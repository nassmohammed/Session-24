{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbd8fea-243b-497f-af7c-71c241b520a3",
   "metadata": {},
   "source": [
    "# Fun with Astrometry\n",
    "\n",
    "**LSST-DA Data Science Fellowship Program**\n",
    "\n",
    "*by Alex Drlica-Wagner (2025-09-15)*\n",
    "\n",
    "In this notebook we explore a few features related to astrometry and proper motion. I've attempted to follow the mathematical notation used in the lecture notes, though you may also want to look at Gary Bernstein's notes from DSFP Session 11.\n",
    "\n",
    "*Acknowledgements: This notebook draws inspiration from previous DSFP notebooks from Gary Bernstein and Colin Slater.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e5659-78ec-40f9-8027-f454fa3ff144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2dd56-d397-4d80-8911-04e6eca791ac",
   "metadata": {},
   "source": [
    "## Exercise 1: Centroid Fitting\n",
    "\n",
    "\n",
    "### The 1D Case\n",
    "\n",
    "In the lecture notes we asserted several statistical properties that we will now verify. First, let's verify that in the trivial, unbinned case, the uncertainty on the mean decreases as $1/\\sqrt{N}$. We will do this by sampling the standard normal distribution (our \"PSF\") multiple times with different numbers of samples (\"photons\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38aa9c-3282-47ed-8252-1cd972477b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the 1D locations \"photons\" from a \"PSF\"\n",
    "\n",
    "# Use scipy.stats to create a Gaussian probability density function (pdf) to represent our PSF.\n",
    "# (Aside: the random variable classes in scipy.stats are very useful for quick simulations)\n",
    "mu,sigma = 0, 1\n",
    "pdf = scipy.stats.norm(mu,sigma)\n",
    "\n",
    "# Measure the mean calculated from 1000 trials at each number of photons\n",
    "ntrials = 1000\n",
    "nphotons = [1, 10, 100, 1000, 10000]\n",
    "unbinned_means = []\n",
    "for i in nphotons:\n",
    "    means = []\n",
    "    for j in range(ntrials):\n",
    "        # Draw \"i\" randome samples from the pdf\n",
    "        x = pdf.rvs(size=i)\n",
    "        # Calculate the mean and add it to our list of means\n",
    "        means.append(np.mean(x))\n",
    "    unbinned_means.append( [i, means] )\n",
    "\n",
    "# Note that an equivalent (and faster) way to do this is\n",
    "unbinned_means = [ (i, np.mean(pdf.rvs(size=(i, ntrials)), axis=0)) for i in nphotons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1d673-1932-4a05-9a29-25e5adbd8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the distribution of means changes with photon number\n",
    "plt.figure()\n",
    "for n, mean in unbinned_means:\n",
    "    plt.hist(mean, bins=np.linspace(-1, 1, 100), label=f\"Number of Photons: {n}\")\n",
    "plt.xlabel(\"Unbinned Mean\")\n",
    "plt.ylabel(\"Number of Trials\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526859ee-6ba0-4b6c-82a6-2dc8343e8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([np.std(means) for nph, means in unbinned_means])\n",
    "plt.scatter(nphotons, std, label=\"Measurements\")\n",
    "plt.plot(nphotons, sigma/np.sqrt(nphotons), '--', color='gray', label=r\"$\\sigma/\\sqrt{N}$\")\n",
    "plt.gca().set_yscale('log')\n",
    "plt.gca().set_xscale('log')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Scatter in the Mean\")\n",
    "plt.xlabel(\"Number of Photons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c0709-78dc-466f-8088-50c2e814d114",
   "metadata": {},
   "source": [
    "The above example demonstrates that our statistical assumptions worked for continuous, unpixelized samples from a distribution. Now we want to check that the same statistical properties hold for a pixelized distribution.\n",
    "\n",
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span>  Use `np.histogram` to bin the \"photons\" into \"pixels\". Verify that our statistical assumption holds for 1D pixelated data when the mean is calculated as $$\\bar{x} = \\frac{ \\sum_{x} x I_x } {\\sum_{x} I_x},$$ where $I_x$ is the intesity (counts) in the pixel located at coordinate $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a78738-5f66-41f2-b977-3a8f575e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the 1D locations \"photons\" from a \"PSF\"\n",
    "mu,sigma = 0, 1\n",
    "pdf = scipy.stats.norm(mu,sigma)\n",
    "\n",
    "# Configure the bins (\"pixels\")\n",
    "bins = np.linspace(-10,10,101)\n",
    "centers = (bins[1:] + bins[:-1])/2.\n",
    "\n",
    "# Create sources with different numbers of photons\n",
    "nphotons = [1, 10, 100, 1000, 10000]\n",
    "# Number of realizations of each source\n",
    "ntrials = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0baf9f-9f94-4237-87f0-07ab627c9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23053321-1249-4430-a9b9-0b720277c894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T21:29:19.067964Z",
     "iopub.status.busy": "2025-09-14T21:29:19.067687Z",
     "iopub.status.idle": "2025-09-14T21:29:19.074169Z",
     "shell.execute_reply": "2025-09-14T21:29:19.073564Z",
     "shell.execute_reply.started": "2025-09-14T21:29:19.067945Z"
    }
   },
   "source": [
    "### The 2D Case\n",
    "\n",
    "Now, let's try the 2D example. First we'll start by defining our 2D point source as a multivariate normal distribution centered at $x,y = (2,0)$ with $\\sigma_x = \\sigma_y = 1$. (We choose to offset the source and make our image rectangular to help double check that our array indexing is correct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8909fde-f573-48a5-a465-f461656442d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2d normal distribution\n",
    "\n",
    "# The mean of the distribution (offset to help verify indexing)\n",
    "xtrue,ytrue = 2, 0\n",
    "mu = np.array([xtrue,ytrue])\n",
    "# The spread (now a covariance array)\n",
    "sigma = 1\n",
    "cov = np.array([ [sigma,0], [0,sigma] ])\n",
    "# Create the multivariate normal \"PSF\" \n",
    "pdf = scipy.stats.multivariate_normal(mean=mu,cov=cov)\n",
    "\n",
    "# Create the pixel edges and centers\n",
    "# (Rectangular image to help verify indexing)\n",
    "bins = xbins, ybins = [np.linspace(-10,10,101), np.linspace(-15,15,151)]\n",
    "centers = xcenters, ycenters = [(bins[0][1:] + bins[0][:-1])/2., (bins[1][1:] + bins[1][:-1])/2.]\n",
    "\n",
    "# x,y coordinates for the center of each pixel\n",
    "# Note the convention of numpy is array[row, column]...\n",
    "# The indexing='ij' is a \"shortcut\" that I \"discovered\". Does it work? I don't know...\n",
    "xx,yy = np.meshgrid(xcenters, ycenters, indexing='ij')\n",
    "\n",
    "# Sanity check for x,y coordinates...\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].pcolormesh(xx,yy,xx)\n",
    "ax[1].pcolormesh(xx,yy,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e91a14-4bd9-4157-9735-f9052dcb376f",
   "metadata": {},
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Generate 1000 photons from our 2D representation of a \"PSF\". Plot the distribution as a scatter plot. Use `np.histogram2d` to bin the photons into pixels. Call the counts in each pixel `Ixy` (the intensity).  Use `pcolormesh` to plot `Ixy` to show what the image produced from a pixelated detector would look like. (I used `plt.pcolormesh(xx,yy,Ixy)`, but any combination that has the correct x,y orientation should work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6bc08-3ba2-4d20-93f3-951ea9bea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd512e-3d2e-49c3-8fcf-68e8a2814eb1",
   "metadata": {},
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Calculate the 2D centroid of your pixelated distribution, $(\\bar{x}, \\bar{y})$, using the counts in each pixel, $I(x,y)$ from the formulae\n",
    "\n",
    "$$\\bar{x} = \\frac{\\sum_{xy} x I_{xy}}{\\sum_{xy} I_{xy}},~~ \\bar{y} = \\frac{\\sum_{xy} y I_{xy}}{\\sum_{xy} I_{xy}}$$\n",
    "\n",
    "Plot the centroid on top of your pixelated image. How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3096c3-2e13-43e7-ab62-1456b1aa76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53b468-b842-4bc4-bd89-fb63a946e8bb",
   "metadata": {},
   "source": [
    "### Adding noise\n",
    "\n",
    "Now we are going to add some random noise to the counts in each pixel. This is akin to the noise that would be left after subtracting the sky background. For the purposes of this exercise, assume that the noise is Gaussian: $\\mathcal{N}(\\mu, \\sigma)$. Note that the noise after background subtraction will fluctuate both positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c724b-35ba-4f51-bf79-12515aeb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random Gaussian noise left over after sky subtraction.\n",
    "sky_mu, sky_sigma = 0, 3\n",
    "sky_noise = scipy.stats.norm(sky_mu, sky_sigma).rvs(size=Ixy.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.pcolormesh(xx, yy, sky_noise)\n",
    "plt.colorbar(label=\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9536a91-36eb-4741-a502-5fd7561218f9",
   "metadata": {},
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Question:</span> Add the sky noise to your pixelated counts. Plot the image of your source + noise and calculate the centroid $(\\bar{x}, \\bar{y})$ following the same procedure above. Do this several times for several realizations of the noise. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874ebe4-eb6d-4e5f-af49-179f7a9904d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc40380-acf2-4ba6-8a7f-98d449b67ac3",
   "metadata": {},
   "source": [
    "### Windowed Centroiding\n",
    "\n",
    "As discussed in the lecture, the common way to perform centroiding in the background-dominated regime is to apply an aperture (or window) filter function. We will define two simple top-hat windows below. Applying these windows will basically amount to restricting our centroid measurement to only use the pixels around the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d36d70-2432-45e1-83ab-125288e5e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the top-hat window array\n",
    "\n",
    "def square_window(x0, y0, half_width):\n",
    "    \"\"\" A square top-hat window.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x0, y0 : center of the window\n",
    "    half_width : half of the width of the square\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    window : the window array\n",
    "    \"\"\"\n",
    "    window = np.zeros_like(Ixy)\n",
    "    window  = (np.abs(xx - x0) < half_width)\n",
    "    window &= (np.abs(yy - y0) < half_width)\n",
    "    return window\n",
    "    \n",
    "def circular_window(x0, y0, radius):\n",
    "    \"\"\" A circular top-hat window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0, y0 : center of the window\n",
    "    radius : radius of the circle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    window : the window array\n",
    "    \"\"\"\n",
    "    window = np.zeros_like(Ixy)\n",
    "    window  = np.sqrt( (xx - x0)**2 + (yy - y0)**2) < radius\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479386a1-fe01-45ad-bbb7-17232f823e33",
   "metadata": {},
   "source": [
    "In order to apply the filter, we need to make an initial guess at where the window should be centered. We've just seen that our centroid estimate can be way off, so we need another estimate. We could use the true center of the object, but that is cheating. Let's try centering the window on the \"peak\", i.e., the pixel with the maximum number of counts (this has worked in most realizations of the noise that I have seen). In a more realistic example, you would have already run source detection, which will have given you a rough estimate for where the source is located.\n",
    "\n",
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Find the x,y coordinates of the pixel with the maximum value. Plot each of the two window functions centered on that pixel with a half-width/radius of `3*sigma`, where `sigma` is the effective \"PSF\" width that we to simulate our source (our results won't be too sensitive to our choice, and we can assume that we have measured this \"PSF\" from bright stars...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908728cd-6e51-4ecf-91d9-4d1f875f4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb562c-b788-4c76-9e34-7a0e79c48e3f",
   "metadata": {},
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Apply the circular window and re-calculate the centroid of your noisy image\n",
    "\n",
    "$$\\bar{x} = \\frac{\\sum_{xy} x I_{xy} w_{xy}}{\\sum_{xy} I_{xy} w_{xy}},~~ \\bar{y} = \\frac{\\sum_{xy} y I_{xy} w_{xy}}{\\sum_{xy} I_{xy} w_{xy}}.$$\n",
    "\n",
    "Did it help? Could you imagine an iterative approach that would converge on a more correct centroid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94256b09-b3f3-4afd-bebf-f3f0240e9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af26055-6ced-4f22-8491-c346571d6a9f",
   "metadata": {},
   "source": [
    "## Excercise 2: Fast Moving Star\n",
    "\n",
    "*(Adapted from Gary Bernstein)*\n",
    "\n",
    "This exercise uses two CCD images and the associated catalogs from the Dark Energy Survey (DES):\n",
    "* First Image: [D00387788_r_c24_r3567p02_immasked.fits.fz](https://data.darkenergysurvey.org/fnalmisc/D00387788_r_c24_r3567p02_immasked.fits.fz)\n",
    "* First Catalog: [D00387788_r_c24_r3567p02_red-fullcat.fits](https://data.darkenergysurvey.org/fnalmisc/D00387788_r_c24_r3567p02_red-fullcat.fits)\n",
    "* Second Image: [D00704796_r_c35_r3518p01_immasked.fits.fz](https://data.darkenergysurvey.org/fnalmisc/D00704796_r_c35_r3518p01_immasked.fits.fz)\n",
    "* Second Catalog: [D00704796_r_c35_r3518p01_red-fullcat.fits](https://data.darkenergysurvey.org/fnalmisc/D00704796_r_c35_r3518p01_red-fullcat.fits)\n",
    "\n",
    "According to Gaia, there is a fast-moving star located at roughly (29.91148,-8.212267). If you display these two images using DS9, then align them using the Frame->Match->Frame->WCS option, you will be able to move your cursor to those coordinates actually see the star move.\n",
    "\n",
    "We are going to practice using the WCS by starting from the windowed centroids of the catalog detections of this star, applying the WCS to transform the centroids to sky coordinates, and calculating the proper motion of the star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991127f-273d-4499-b0bf-391f455c025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common astropy imports\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91361f9f-fd87-4ec8-aaaf-9d9c85f74491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the FITS files. This returns HDU lists.\n",
    "\n",
    "img1 = fits.open('D00387788_r_c24_r3567p02_immasked.fits.fz')\n",
    "cat1 = fits.open('D00387788_r_c24_r3567p02_red-fullcat.fits')\n",
    "\n",
    "img2 = fits.open('D00704796_r_c35_r3518p01_immasked.fits.fz')\n",
    "cat2 = fits.open('D00704796_r_c35_r3518p01_red-fullcat.fits')\n",
    "\n",
    "# Print the contents of the files\n",
    "print(\"Image file:\")\n",
    "[print(hdu) for hdu in img1] \n",
    "print(\"Catalog file:\")\n",
    "[print(hdu) for hdu in cat1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc86747-09c3-4d64-a6ee-d2c3056d9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the header, image, and catalog data as shown below...\n",
    "header = img1[1].header\n",
    "image = img1[1].data\n",
    "catalog = cat1[2].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29535934-f76c-4a8a-bc18-91fd78521191",
   "metadata": {},
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> For each of the two images, get the Modified Julien Date (`MJD-OBS`) from the header and use the header to create a World Coordinate System (WCS). How far separated in time are the two images? How far separated in angle on the sky are the centers of the two CCDs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a37b40-27f0-442c-afc3-2f3b30e01bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6235131-111a-4222-8aa3-08759257abda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:58:53.226770Z",
     "iopub.status.busy": "2025-09-16T14:58:53.226444Z",
     "iopub.status.idle": "2025-09-16T14:58:53.234189Z",
     "shell.execute_reply": "2025-09-16T14:58:53.233650Z",
     "shell.execute_reply.started": "2025-09-16T14:58:53.226746Z"
    }
   },
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Use the associated WCS to convert the windowed centroids of the catalogs into sky coordinates (RA, Dec). Find the catalog object associated with our target star at RA, DEC ~ (29.91148,-8.212267). What the the windowed centroid coordinates (`XWIN_IMAGE`, `YWIN_IMAGE`) for the detection? What is the major axis of the windowed image centroid error ellipse (`ERRAWIN_IMAGE`) that we will use as an estimate of the statistical uncertainty on the centroid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1a03a-72fa-4f5f-8a14-22a96a949fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa6eb0-31a0-41ad-b409-e21597ebfbea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T15:08:14.775835Z",
     "iopub.status.busy": "2025-09-16T15:08:14.775477Z",
     "iopub.status.idle": "2025-09-16T15:08:14.823885Z",
     "shell.execute_reply": "2025-09-16T15:08:14.823127Z",
     "shell.execute_reply.started": "2025-09-16T15:08:14.775801Z"
    }
   },
   "source": [
    "<span><span style=\"color:blue;font-weight: bold;\">Exercise:</span> Using the two sky coordinates calculated above to estimate the proper motion (in mas/yr) of the star between the two exposures. Note that proper motions are traditionally given in real angular motions, so that (RA PM) = (difference in RA) * cos(dec) / (time interval). Estimate the uncertainty on these measurements using the `ERRAWIN_IMAGE` estimates and a 10 mas turbulence uncertainty (added in quadrature).\n",
    "\n",
    "Compare your measurement to the measurement from Gaia DR2:\n",
    "```\n",
    "pmra  =  326.06  +/- 0.94 mas/yr\n",
    "pmdec = -124.12 +/- 0.81 mas/yr\n",
    "```\n",
    "\n",
    "Can you speculate about some inaccuracies in your estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ff55d-2aed-4306-a4ef-183b469784f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
